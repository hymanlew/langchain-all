"""
åº”è¯¥ä½¿ç”¨ create_tool_calling_agent è€Œé create_react_agentï¼Œ
- å‰è€…å·¥å…·è°ƒç”¨ä»£ç†æ›´é€‚åˆä¼ä¸šåœºæ™¯ï¼Œå®ƒæ˜ç¡®åŒºåˆ†å·¥å…·ä½¿ç”¨å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œ
- åè€… Reactä»£ç†æ›´é€‚åˆç ”ç©¶åœºæ™¯ï¼Œåœ¨ä¼ä¸šåœºæ™¯ä¸­å¯èƒ½å¯¼è‡´ä¸å¯é¢„æµ‹çš„è¡Œä¸º

å³ä½¿è¦ä½¿ç”¨ Reactæ¨¡å¼ï¼Œä¹Ÿè¦ç”¨ from langchain.agents import create_react_agentï¼Œä»¥ä¿è¯å®æ—¶æ›´æ–°
"""
from typing import List, Dict, Optional
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_mcp_adapters.client import MultiServerMCPClient
import gradio as gr
import logging
from mcp_config import Config
from fastapi import HTTPException
from tenacity import retry, stop_after_attempt, wait_exponential


# åˆå§‹åŒ–LLMï¼Œæœ¬åœ°éƒ¨ç½²çš„
llm = ChatopenAI(
	temperature=0,
	model="qwen3-8b",
	api_key="EMPTY",
	api_base="http://localhost:6006/v1"
	# å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼
	extra_body={"chat_template_kwargs": {"enable_thinking": True}},
)


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ç³»ç»Ÿæç¤ºæ¨¡æ¿
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå°½å¯èƒ½çš„è°ƒç”¨å·¥å…·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
è¯·éµå¾ªä»¥ä¸‹è§„åˆ™:
1. ç¡®ä¿å›ç­”å‡†ç¡®ã€ä¸“ä¸š
2. å¯¹äºä¸ç¡®å®šçš„ä¿¡æ¯æ˜ç¡®è¯´æ˜
3. éµå®ˆä¼ä¸šæ•°æ®å®‰å…¨æ”¿ç­–
4. é¿å…æä¾›æ•æ„Ÿä¿¡æ¯"""

prompt = ChatPromptTemplate.from_messages([
    ('system', SYSTEM_PROMPT),
    MessagesPlaceholder(variable_name='chat_history', optional=True),
    ('human', '{input}'),
    MessagesPlaceholder(variable_name='agent_scratchpad', optional=True),
])


"""
asynccontextmanager å®é™…æ˜¯é€šè¿‡ MultiServerMCPClient çš„å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥å£éšå¼è°ƒç”¨çš„ï¼Œä¼ä¸šçº§ä»£ç ä¸­åº”é¿å…ç›´æ¥æ“ä½œåº•å±‚å¼‚æ­¥åŸè¯­  
asyncio ä½œä¸ºè¿è¡Œæ—¶åŸºç¡€ä¾èµ–ï¼Œåº”ç”±æ¡†æ¶å±‚ï¼ˆå¦‚ `langchain_mcp_adapters`ï¼‰ç»Ÿä¸€ç®¡ç†ï¼Œè€Œéä¸šåŠ¡ä»£ç æ˜¾å¼å¼•å…¥ã€‚
ç›´æ¥ä½¿ç”¨ `asyncio` å¯èƒ½å¯¼è‡´çº¿ç¨‹å®‰å…¨é—®é¢˜ï¼Œè€Œæ¡†æ¶æä¾›çš„å®¢æˆ·ç«¯ï¼ˆå¦‚ `MultiServerMCPClient`ï¼‰å·²å®ç°çº¿ç¨‹å®‰å…¨çš„å¼‚æ­¥å°è£…

ä»…åœ¨ä»¥ä¸‹åœºæ™¯ä¿ç•™ç›´æ¥å¯¼å…¥ï¼ˆå…¶ä»–æƒ…å†µåº”ä¼˜å…ˆä½¿ç”¨æ¡†æ¶æä¾›çš„å¼‚æ­¥æŠ½è±¡ï¼‰ï¼š
1. **ç¼–å†™åŸºç¡€è®¾æ–½ç»„ä»¶**ï¼ˆå¦‚è‡ªå®šä¹‰è¿æ¥æ± ï¼‰  
2. **æ€§èƒ½å…³é”®å‹ä»£ç **éœ€ç²¾ç»†æ§åˆ¶äº‹ä»¶å¾ªç¯ç­–ç•¥  
3. **å…¼å®¹æ—§ç‰ˆPython**ï¼ˆ<3.7éœ€`@asyncio.coroutine`ï¼‰  

# åœ¨åŸºç¡€è®¾æ–½å±‚é›†ä¸­ç®¡ç†ï¼ˆå¦‚ async_utils.pyï¼‰ï¼Œè‹¥ç¡®å®éœ€è¦è‡ªå®šä¹‰å¼‚æ­¥é€»è¾‘ï¼Œåº”é‡‡ç”¨ä»¥ä¸‹æ¨¡å¼ï¼š
from contextlib import asynccontextmanager
import asyncio

# å¼‚æ­¥ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆappï¼‰ï¼Œå¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨åœ¨è¿›å…¥å’Œé€€å‡ºä¸Šä¸‹æ–‡æ—¶å¯ä»¥æ‰§è¡Œå¼‚æ­¥æ“ä½œã€‚
@asynccontextmanager
async def managed_client(config: dict):
    """ä¼ä¸šçº§å°è£…çš„å¼‚æ­¥å®¢æˆ·ç«¯"""
    async with MultiServerMCPClient(config) as client:
        try:
            yield client
        except asyncio.TimeoutError:
            logger.error("MCP client timeout")
            raise ServiceUnavailableError()
"""

# åœ¨ agent ä¸­è¿æ¥ MCP_SERVER æ—¶ï¼Œå¿…é¡»æ˜¯åœ¨å¼‚æ­¥ç¯å¢ƒä¸‹å»ºç«‹è¿æ¥çš„
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def execute_graph(chat_bot: List[Dict]) -> List[Dict]:
    """æ‰§è¡Œå·¥ä½œæµçš„å‡½æ•°ï¼Œå¢åŠ é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†"""
    try:
        user_input = chat_bot[-1]['content']
        if not user_input.strip():
            raise ValueError("Empty user input")
            
        inputs = {"input": user_input}
        
		"""
		async def çš„ä½œç”¨æ˜¯å£°æ˜å‡½æ•°ä¸ºåç¨‹å‡½æ•°ï¼Œä½¿å…¶å†…éƒ¨å¯ä»¥åŒ…å« awaitã€async with ç­‰å¼‚æ­¥æ“ä½œã€‚ä½†å‡½æ•°æœ¬èº«çš„å®šä¹‰ä¸ä¼šè‡ªåŠ¨ä½¿å…¶å†…éƒ¨ä»£ç å¼‚æ­¥æ‰§è¡Œã€‚
		- è‹¥å†…éƒ¨ä»£ç éœ€è¦å¼‚æ­¥æ‰§è¡Œï¼ˆå¦‚æ•°æ®åº“è¿æ¥å»ºç«‹ã€èµ„æºé‡Šæ”¾ã€ç½‘ç»œä¼šè¯ç­‰ï¼‰ï¼Œåˆ™å¿…é¡»ç”¨ async with æ¥è®©å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç®¡ç†å…¶ç”Ÿå‘½å‘¨æœŸï¼ˆæ™®é€šwithä¼šé˜»å¡äº‹ä»¶å¾ªç¯ï¼‰ã€‚
		- è‹¥å†…éƒ¨ä»£ç æ“ä½œå¯¹è±¡æ˜¯åŒæ­¥çš„ï¼ˆå¦‚æœ¬åœ°è®¡ç®—ã€éå¼‚æ­¥I/Oï¼‰ï¼Œåˆ™æ— éœ€åŠ  asyncã€‚
		"""
		# MultiServerMCPClient å¯ä»¥æ¥æ”¶å¤šä¸ª server é…ç½®ï¼Œå³å¯ä»¥è¿æ¥å¤šä¸ª MCP æœåŠ¡å™¨
		# with è‡ªåŠ¨é‡Šæ”¾èµ„æº
        async with MultiServerMCPClient(Config.MCP_SERVER_CONFIG) as client:
            tools = client.get_tools()
            logger.info(f"Available tools: {[t.name for t in tools]}")
            
            # agent = create_react_agent(llm, client.get_tools())
            # ä½¿ç”¨å·¥å…·è°ƒç”¨ä»£ç†è€ŒéReactä»£ç†ï¼Œæ›´é€‚åˆä¼ä¸šåœºæ™¯
			agent = create_tool_calling_agent(llm, tools, prompt)
            executor = AgentExecutor(
                agent=agent, 
                tools=tools,
                handle_parsing_errors=True,
                max_iterations=10  # é™åˆ¶è¿­ä»£æ¬¡æ•°é˜²æ­¢æ— é™å¾ªç¯
            )
            
            response = await executor.ainvoke(input=inputs)
            result = response["output"]
            
            # è®°å½•äº¤äº’å†å²
            logger.info(f"User: {user_input}\nAssistant: {result}")
            
            chat_bot.append({'role': 'assistant', 'content': result})
            return chat_bot
            
    except Exception as e:
        logger.error(f"Error in execute_graph: {str(e)}", exc_info=True)
        chat_bot.append({
            'role': 'assistant', 
            'content': "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°é—®é¢˜ã€‚æˆ‘ä»¬çš„æŠ€æœ¯å›¢é˜Ÿå·²æ”¶åˆ°é€šçŸ¥ã€‚"
        })
        return chat_bot
		
def do_graph(user_input: str, chat_bot: List[Dict]) -> tuple:
    """è¾“å…¥å¤„ç†å‡½æ•°ï¼Œå¢åŠ è¾“å…¥éªŒè¯"""
    if user_input and user_input.strip():
        # ç®€å•çš„å†…å®¹è¿‡æ»¤
        if any(word in user_input.lower() for word in ["å¯†ç ", "æ•æ„Ÿ", "æœºå¯†"]):
            chat_bot.append({
                'role': 'assistant',
                'content': "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†åŒ…å«æ•æ„Ÿä¿¡æ¯çš„è¯·æ±‚ã€‚"
            })
            return '', chat_bot
            
        chat_bot.append({'role': 'user', 'content': user_input.strip()})
    return '', chat_bot


with gr.Blocks(title='è°ƒç”¨MCPæœåŠ¡çš„Agenté¡¹ç›®', css=Config.CSS) as instance:
    gr.Label('è°ƒç”¨MCPæœåŠ¡çš„Agenté¡¹ç›®', container=False)

    chatbot = gr.Chatbot(type='messages', height=450, label='AIå®¢æœ')  # èŠå¤©è®°å½•ç»„ä»¶

    input_textbox = gr.Textbox(label='è¯·è¾“å…¥ä½ çš„é—®é¢˜ğŸ“', value='')  # è¾“å…¥æ¡†ç»„ä»¶

    input_textbox.submit(do_graph, [input_textbox, chatbot], [input_textbox, chatbot])
	.then(execute_graph, chatbot, chatbot)


if __name__ == '__main__':
    # ç”Ÿäº§ç¯å¢ƒå¯åŠ¨
    instance.launch(**{
        "auth": Config.GRADIO_AUTH,
        "server_name": "0.0.0.0",
        "server_port": 7860,
        "share": False,
        "debug": False
    })
	
	
	