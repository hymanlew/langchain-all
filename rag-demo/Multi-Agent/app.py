# models.py
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore

def load_model(model_name: str) -> ChatOllama:
    """
    åŠ è½½è¯­è¨€æ¨¡å‹
    å‚æ•°:
        model_name (str): æ¨¡å‹åç§°
    è¿”å›:
        ChatOllamaå®ä¾‹ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬å’Œå›ç­”é—®é¢˜
    """
    return ChatOllama(model=model_name)
    
def load_embeddings(model_name: str) -> OllamaEmbeddings:
    """
    åŠ è½½åµŒå…¥æ¨¡å‹

    å‚æ•°:
        model_name (str): æ¨¡å‹åç§°

    è¿”å›:
        OllamaEmbeddingså®ä¾‹ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
    """
    return OllamaEmbeddings(model=model_name)

def load_vector_store(model_name: str) -> InMemoryVectorStore:
    """
    åˆ›å»ºå†…å­˜å‘é‡å­˜å‚¨

    å‚æ•°:
        model_name (str): ç”¨äºç”ŸæˆåµŒå…¥çš„æ¨¡å‹åç§°

    è¿”å›:
        InMemoryVectorStoreå®ä¾‹ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡åŒ–çš„æ–‡æœ¬
    """
    embeddings = load_embeddings(model_name)
    return InMemoryVectorStore(embeddings)



# summary.py
from langchain.prompts import ChatPromptTemplate
from chains.models import load_model

class SummaryChain:
    """
    ä¸€ä¸ªç”¨äºç”Ÿæˆæœç´¢æŸ¥è¯¢çš„ç±»ã€‚
    å®ƒä»ç”¨æˆ·é—®é¢˜å’ŒèŠå¤©è®°å½•ä¸­æå–å…³é”®è¯ï¼Œå¹¶ç”Ÿæˆé«˜æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚
    """
    def __init__(self, model_name):
        """
        åˆå§‹åŒ– SummaryChain ç±»ï¼Œå¹¶åŠ è½½æŒ‡å®šçš„è¯­è¨€æ¨¡å‹ã€‚
        å‚æ•°:
            model_name (str): è¦åŠ è½½çš„è¯­è¨€æ¨¡å‹çš„åç§°ã€‚
        """
        self.llm = load_model(model_name)
        self.prompt = ChatPromptTemplate.from_template(
            "You are a professional assistant specializing in extracting keywords from user questions and chat histories. 
			Extract keywords and connect them with spaces to output a efficient and precise search query. Be careful not answer the question directly, 
			just output the search query.\n\nHistories: {history}\n\nQuestion: {question}"
        )
        self.chain = self.prompt | self.llm

    def invoke(self, input_data):
        """
        ä½¿ç”¨æä¾›çš„è¾“å…¥æ•°æ®è°ƒç”¨é“¾ä»¥ç”Ÿæˆæœç´¢æŸ¥è¯¢ã€‚

        å‚æ•°:
            input_data (dict): åŒ…å« 'history' å’Œ 'question' é”®çš„å­—å…¸ã€‚

        è¿”å›:
            str: é“¾ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢ã€‚
        """
        return self.chain.invoke(input_data)



# generate.py
from langchain.prompts import ChatPromptTemplate
from chains.models import load_model

class GenerateChain:
    """
    ä¸€ä¸ªç”¨äºç”Ÿæˆé—®ç­”ä»»åŠ¡å“åº”çš„ç±»ã€‚
    å®ƒä½¿ç”¨è¯­è¨€æ¨¡å‹å’Œæç¤ºæ¨¡æ¿æ¥å¤„ç†è¾“å…¥æ•°æ®ã€‚
    """
    def __init__(self, model_name):
        """
        åˆå§‹åŒ– GenerateChain ç±»ï¼Œå¹¶åŠ è½½æŒ‡å®šçš„è¯­è¨€æ¨¡å‹ã€‚

        å‚æ•°:
            model_name (str): è¦åŠ è½½çš„è¯­è¨€æ¨¡å‹çš„åç§°ã€‚
        """
        self.llm = load_model(model_name)
        self.prompt = ChatPromptTemplate.from_template(
		"You are an assistant for question-answering tasks. Use the following documents or chat histories to answer the question. If the documents or chat histories is empty, 
		answer the question based on your own knowledge. If you don't know the answer, just say that you don't know.\n\nDocuments: {documents}\n\nHistories: {history}\n\nQuestion: {question}"
		)
        self.chain = self.prompt | self.llm

    def invoke(self, input_data):
        """
        ä½¿ç”¨æä¾›çš„è¾“å…¥æ•°æ®è°ƒç”¨é“¾ä»¥ç”Ÿæˆå“åº”ã€‚

        å‚æ•°:
            input_data (dict): åŒ…å« 'documents'ã€'history' å’Œ 'question' é”®çš„å­—å…¸ã€‚

        è¿”å›:
            str: é“¾ç”Ÿæˆçš„å“åº”ã€‚
        """
        return self.chain.invoke(input_data)



# æ™ºèƒ½ä½“ä¹‹é—´çš„è¿æ¥é€šè¿‡çŠ¶æ€å›¾ï¼ˆgraphï¼‰æ¥å®ç°ï¼Œä½¿ç”¨çŠ¶æ€ï¼ˆstateï¼‰å­˜å‚¨äº¤äº’çš„ä¿¡æ¯ã€‚å›¾ç”±èŠ‚ç‚¹ï¼ˆnodeï¼‰å’Œè¾¹ï¼ˆedgeï¼‰ç»„æˆï¼ŒèŠ‚ç‚¹è¡¨ç¤ºæ™ºèƒ½ä½“ï¼Œè¾¹è¡¨ç¤ºæ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»ã€‚
# graph_state.py
from typing import Literal, Annotated, Optional
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

class GraphState(TypedDict):
    """å®šä¹‰å›¾çŠ¶æ€çš„ç±»å‹å­—å…¸ã€‚ç”¨äºè¡¨ç¤ºå›¾ä¸­çš„çŠ¶æ€ä¿¡æ¯ã€‚"""
    model_name: str  # ä½¿ç”¨çš„æ¨¡å‹åç§°
    type: Literal["websearch", "file", "chat"]  # æ“ä½œç±»å‹ï¼ŒåŒ…æ‹¬è”ç½‘æœç´¢ã€ä¸Šä¼ æ–‡ä»¶å’ŒèŠå¤©
    messages: Annotated[list, add_messages]  # æ¶ˆæ¯åˆ—è¡¨ï¼Œä½¿ç”¨add_messagesæ³¨è§£å¤„ç†æ¶ˆæ¯è¿½åŠ 
    documents: Optional[list] = []  # æ–‡æ¡£åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©ºåˆ—è¡¨



# å®šä¹‰äº†å¤šä¸ªæ–¹æ³•ï¼Œè¡¨ç¤ºå›¾çš„ç»“æ„å’Œè¡Œä¸ºï¼Œç”¨äºå¤„ç†ä¸åŒç±»å‹çš„è¯·æ±‚
# graph.py
import os
from langchain.schema import Document
from langchain_core.runnables import RunnableConfig
from langchain_community.document_loaders import TextLoader
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langgraph.graph.state import StateGraph, CompiledStateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered
from graph.graph_state import GraphState
from chains.summary import SummaryChain
from chains.generate import GenerateChain

def route_question(state: GraphState) -> str:
    """
    æ ¹æ®æ“ä½œç±»å‹è·¯ç”±åˆ°ç›¸åº”çš„å¤„ç†èŠ‚ç‚¹ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        str: ä¸‹ä¸€ä¸ªè¦è°ƒç”¨çš„èŠ‚ç‚¹åç§°
    """
    print("--- ROUTE QUESTION ---")
    if state['type'] == 'websearch':
        print("--- ROUTE QUESTION TO EXTRACT KEYWORDS ---")
        return "extract_keywords"
    if state['type'] == 'file':
        print("--- ROUTE QUESTION TO FILE PROCESS ---")
        return "file_process"
    elif state['type'] == 'chat':
        print("--- ROUTE QUESTION TO GENERATE ---")
        return "generate"


# ä¹Ÿå¯ä»¥å°†è·¯ç”±äº¤ç»™ LLM å†³å®šï¼Œåªéœ€è¦å†™å¥½ç›¸åº”çš„æç¤ºè¯å³å¯ï¼Œä¾‹å¦‚ä¸‹é¢çš„æç¤ºè¯å°†ç”± LLM å†³å®šæ˜¯è¿›è¡ŒçŸ¥è¯†åº“æŸ¥è¯¢è¿˜æ˜¯ç½‘ç»œæœç´¢ã€‚
from langchain_core.output_parsers import JsonOutputParser

prompt = ChatPromptTemplate.from_template("You are an expert at routing a user question to a vectorstore or web search. Use the vectorstore for questions on LangChain and LangGraph. You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and no premable or explaination. Question to route: {question}")
router = prompt | llm | JsonOutputParser()

source = router.invoke({"question": question})
if source['datasource'] == 'web_search':
    # TODO: route to web search
elif source['datasource'] == 'vectorstore':
    # TODO: route to vectorstore


def generate(state: GraphState) -> GraphState:
    """
    æ ¹æ®æ–‡æ¡£å’Œå¯¹è¯å†å²ç”Ÿæˆç­”æ¡ˆã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        state (GraphState): è¿”å›æ·»åŠ äº†LLMç”Ÿæˆå†…å®¹çš„æ–°çŠ¶æ€
    """

    print("--- GENERATE ---")
    chain = GenerateChain(state["model_name"])
    messages = state["messages"]
    state["messages"] = chain.invoke({"question": messages[-1].content, "history": messages[:-1], "documents": state["documents"]})
    return state


# å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œæå–æ–‡æœ¬å†…å®¹å¹¶è¿›è¡Œè¯åµŒå…¥ï¼ˆembeddingï¼‰ï¼Œç„¶åå°†å‘é‡å­˜å‚¨è‡³å†…å­˜æ•°æ®åº“ä¸­ã€‚config æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå­˜å‚¨ LLM è¿è¡Œæ—¶çš„é…ç½®å‚æ•°ï¼Œä¼šåœ¨è°ƒç”¨ LLM æ—¶ä¼ å…¥ã€‚
def file_process(state: GraphState, config: RunnableConfig) -> GraphState:
    """
    å¤„ç†æ–‡ä»¶ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
        config (RunnableConfig): å¯è¿è¡Œé…ç½®

    è¿”å›:
        state (GraphState): è¿”å›å›¾çŠ¶æ€ï¼Œå°†æ–‡æ¡£æ·»åŠ  config ä¸­çš„å‘é‡å­˜å‚¨
    """
    print("--- FILE PROCESS ---")
    vector_store = config["configurable"]["vectorstore"]

    for doc in state["documents"]:
        file_path: str = doc.page_content
        if os.path.exists(file_path):
            split_docs: list[Document] = None
            if file_path.endswith(".txt") or file_path.endswith(".md"):
                # å¤„ç†æ–‡æœ¬æˆ–Markdownæ–‡ä»¶
                docs = TextLoader(file_path, autodetect_encoding=True).load()
                # æ–‡æœ¬åˆ†å‰²
                splitter = RecursiveCharacterTextSplitter(separators=["\n\n", "\n", " ", ".", ",", "\u200B", "\uff0c", "\u3001", "\uff0e", "\u3002", ""], chunk_size=1000, chunk_overlap=100, add_start_index=True)
                split_docs = splitter.split_documents(docs)
            else: 
                # ä½¿ç”¨ marker-pdf å¤„ç†å…¶ä»–æ–‡ä»¶
                converter = PdfConverter(artifact_dict=create_model_dict())
                rendered = converter(file_path)
                docs, _, _ = text_from_rendered(rendered)
                splitter = MarkdownHeaderTextSplitter([("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")], strip_headers = False)
                split_docs = splitter.split_text(docs)
            # å°†å¤„ç†åçš„æ–‡æ¡£æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­
            vector_store.add_documents(split_docs)
    return state


def extract_keywords(state: GraphState, config: RunnableConfig) -> GraphState:
    """
    ä»é—®é¢˜ä¸­æå–å…³é”®è¯ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
        config (RunnableConfig): å¯è¿è¡Œé…ç½®

    è¿”å›:
        state (GraphState): è¿”å›æ·»åŠ äº†æå–å…³é”®è¯çš„æ–°çŠ¶æ€
    """
    print("--- EXTRACT KEYWORDS ---")
    chain = SummaryChain(state["model_name"])
    messages = state["messages"]
    query = chain.invoke({"question": messages[-1].content, "history": messages[:-1]})
    print(query.content)

    if state["type"] == "websearch":
        # å°†ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­ï¼Œä¸‹ä¸€ä¸ªèŠ‚ç‚¹å°†ä¼šä½¿ç”¨
        state["messages"] = query
    elif state["type"] == "file":
        # ä½¿ç”¨ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
        docs = config["configurable"]["vectorstore"].max_marginal_relevance_search(query.content)
        state["documents"] = docs
    return state


# å¯¹äºâ€œä¸Šä¼ æ–‡ä»¶â€ï¼Œâ€œæå–å…³é”®è¯â€æ—¶å·²ç»è¿›è¡Œäº†æŸ¥è¯¢å¤„ç†ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œâ€œç”Ÿæˆå›ç­”â€ï¼›å¯¹äºâ€œç½‘ç»œæœç´¢â€ï¼Œâ€œæå–å…³é”®è¯â€è¿›è¡Œæœç´¢åï¼Œæ‰èƒ½è¿›è¡Œâ€œç”Ÿæˆå›ç­”â€ã€‚
# æ‰§è¡Œè·¯å¾„ä¸åŒï¼Œè¿˜éœ€è¦è¿›è¡Œåˆ¤æ–­ã€‚
def decide_to_generate(state: GraphState) -> str:
    """
    å†³å®šæ˜¯è¿›è¡Œç½‘ç»œæœç´¢è¿˜æ˜¯ç›´æ¥ç”Ÿæˆå›ç­”ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        str: ä¸‹ä¸€ä¸ªè¦è°ƒç”¨çš„èŠ‚ç‚¹åç§°
    """
    if state["type"] == "websearch":
        print("--- DECIDE TO WEB SEARCH ---")
        return "websearch"
    elif state["type"] == "file":
        print("--- DECIDE TO GENERATE ---")
        return "generate"


def web_search(state: GraphState) -> GraphState:
    """
    åŸºäºé—®é¢˜è¿›è¡Œç½‘ç»œæœç´¢ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        state (GraphState): è¿”å›æ·»åŠ äº†ç½‘ç»œæœç´¢ç»“æœçš„æ–°çŠ¶æ€
    """

    print("--- WEB SEARCH ---")
    web_search_tool = TavilySearchResults(k=3)
    documents = state["documents"]
    try:
        docs = web_search_tool.invoke({"query": state["messages"][-1].content})
        web_results = "\n".join([d["content"] for d in docs])
        web_results = Document(page_content=web_results)
        documents.append(web_results)
        state["documents"] = documents
    except:
        pass
    return state


def create_graph() -> CompiledStateGraph:
    """
    åˆ›å»ºå¹¶é…ç½®çŠ¶æ€å›¾å·¥ä½œæµã€‚

    è¿”å›:
        CompiledStateGraph: ç¼–è¯‘å¥½çš„çŠ¶æ€å›¾
    """

    workflow = StateGraph(GraphState)
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("websearch", web_search)
    workflow.add_node("extract_keywords", extract_keywords)
    workflow.add_node("file_process", file_process)
    workflow.add_node("generate", generate)
    # æ·»åŠ è¾¹
    workflow.set_conditional_entry_point(
        route_question,
        {
            "extract_keywords": "extract_keywords",
            "generate": "generate",
            "file_process": "file_process",
        },
    )
    workflow.add_edge("file_process", "extract_keywords")
    workflow.add_conditional_edges(
        "extract_keywords",
        decide_to_generate,
        {
            "websearch": "websearch",
            "generate": "generate",
        },
    )
    workflow.add_edge("websearch", "generate")
    workflow.add_edge("generate", END)

    # åˆ›å»ºå›¾ï¼Œå¹¶ä½¿ç”¨ `MemorySaver()` åœ¨å†…å­˜ä¸­ä¿å­˜çŠ¶æ€
    return workflow.compile(checkpointer=MemorySaver())


def stream_graph_updates(graph: CompiledStateGraph, user_input: GraphState, config: dict):
    """
    æµå¼å¤„ç†å›¾æ›´æ–°å¹¶è¿”å›æœ€ç»ˆç»“æœã€‚

    å‚æ•°:
        graph (CompiledStateGraph): ç¼–è¯‘å¥½çš„çŠ¶æ€å›¾
        user_input (GraphState): ç”¨æˆ·è¾“å…¥çš„çŠ¶æ€
        config (dict): é…ç½®å­—å…¸

    è¿”å›:
        generator: ç”Ÿæˆå™¨å¯¹è±¡ï¼Œé€æ­¥è¿”å›å›¾æ›´æ–°çš„å†…å®¹
    """

    for chunk, _ in graph.stream(user_input, config, stream_mode="messages"):
        yield chunk.content


# åœ¨ main.py æ–‡ä»¶ä¸­ï¼Œå®šä¹‰äº†ä¸€ä¸ªå‘½ä»¤è¡Œç¨‹åºï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è¾“å…¥é—®é¢˜ä¸æ™ºèƒ½ä½“è¿›è¡Œäº¤äº’ã€‚
# main.py
import uuid
from dotenv import load_dotenv
from langchain.schema import Document
from langchain_core.messages import AIMessage, HumanMessage
from chains.models import load_vector_store
from graph.graph import create_graph, stream_graph_updates, GraphState

def main():
    # langchain.debug = True  # å¯ç”¨langchainè°ƒè¯•æ¨¡å¼ï¼Œå¯ä»¥è·å¾—å¦‚å®Œæ•´æç¤ºè¯ç­‰ä¿¡æ¯
    load_dotenv(verbose=True)  # åŠ è½½ç¯å¢ƒå˜é‡é…ç½®

    # åˆ›å»ºçŠ¶æ€å›¾ä»¥åŠå¯¹è¯ç›¸å…³çš„è®¾ç½®
    config = {"configurable": {"thread_id": uuid.uuid4().hex, "vectorstore": load_vector_store("nomic-embed-text")}}  
    state = GraphState(
        model_name="qwen2.5:7b",
        type="chat",
        documents=[Document(page_content="upload_files/test.pdf")],
    )
    graph = create_graph()

    # å¯¹è¯
    while True:
        user_input = input("User: ")
        if user_input.lower() in ["exit", "quit"]:
            break
        state["messages"] = HumanMessage(user_input)
        # æµå¼è·å–AIçš„å›å¤
        for answer in stream_graph_updates(graph, state, config):
            print(answer, end="")
        print()

    # æ‰“å°å¯¹è¯å†å²
    print("\nHistory: ")
    for message in graph.get_state(config).values["messages"]:
        if isinstance(message, AIMessage):
            prefix = "AI"
        else:
            prefix = "User"
        print(f"{prefix}: {message.content}")

if __name__ == "__main__":
    main()


# ä½¿ç”¨ Streamlit æ„å»ºäº†ä¸€ä¸ªç®€å•çš„å‰ç«¯ç•Œé¢ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è¾“å…¥æ¡†ä¸æ™ºèƒ½ä½“è¿›è¡Œäº¤äº’
# app.py 
import uuid
import datetime
from dotenv import load_dotenv
from langchain.schema import Document
import streamlit as st
from streamlit_extras.bottom_container import bottom
from chains.models import load_vector_store
from graph.graph import create_graph, stream_graph_updates, GraphState

# è®¾ç½®ä¸Šä¼ æ–‡ä»¶çš„å­˜å‚¨è·¯å¾„
file_path = "upload_files/"
# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(verbose=True)

def upload_pdf(file):
    """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
    with open(file_path + file.name, "wb") as f:
        f.write(file.getbuffer())
        return file_path + file.name

# è®¾ç½®é¡µé¢é…ç½®ä¿¡æ¯
st.set_page_config(
    page_title="AI-Powerwd Assistant",
    page_icon="ğŸŒ",
    layout="wide"
)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å˜é‡ï¼Œåˆ›å»ºå›¾
if "graph" not in st.session_state:
    st.session_state.graph = create_graph()
# åˆå§‹åŒ–ä¼šè¯IDå’Œå‘é‡å­˜å‚¨
if "config" not in st.session_state:
    st.session_state.config = {"configurable": {"thread_id": uuid.uuid4().hex, "vectorstore": load_vector_store("nomic-embed-text")}}
# åˆå§‹åŒ–å¯¹è¯å†å²è®°å½•
if "history" not in st.session_state:
    st.session_state.history = []
# åˆå§‹åŒ–ä¸Šä¼ çŠ¶æ€ã€æ¨¡å‹åç§°å’Œå¯¹è¯ç±»å‹
if "settings" not in st.session_state:
    st.session_state.settings = {"uploaded": False, "model_name": "qwen2.5:7b", "type": "chat"}

# æ˜¾ç¤ºåº”ç”¨æ ‡é¢˜
st.header("ğŸ‘½ AI-Powerwd Assistant")

# å®šä¹‰å¯é€‰çš„æ¨¡å‹
model_options = {"é€šä¹‰åƒé—® 2.5 7B": "qwen2.5:7b", "DeepSeek R1 7B": "deepseek-r1:7b"}
with st.sidebar:
    # ä¾§è¾¹æ è®¾ç½®éƒ¨åˆ†
    st.header("è®¾ç½®")
    # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
    st.session_state.settings["model_name"] = model_options[st.selectbox("é€‰æ‹©æ¨¡å‹", model_options, index=list(model_options.values()).index(st.session_state.settings["model_name"]))]

    st.divider()

    # æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯
    st.text(f"{datetime.datetime.now().strftime('%Y.%m.%d')} - ZHANG GAOXING")

# å®šä¹‰å¯¹è¯ç±»å‹é€‰é¡¹
type_options = {"ğŸ¤– å¯¹è¯": "chat", "ğŸ” è”ç½‘æœç´¢": "websearch", "ğŸ‘¾ ä»£ç æ¨¡å¼": "code"}
question = None
with bottom():
    # åº•éƒ¨å®¹å™¨ï¼ŒåŒ…å«å·¥å…·é€‰æ‹©ã€æ–‡ä»¶ä¸Šä¼ å’Œè¾“å…¥æ¡†
    st.session_state.settings["type"] = type_options[st.radio("å·¥å…·é€‰æ‹©", type_options.keys(), horizontal=True, label_visibility="collapsed", index=list(type_options.values()).index(st.session_state.settings["type"]))]
    # æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["pdf", "docx", "xlsx", "txt", "md"], accept_multiple_files=False, label_visibility="collapsed")
    # èŠå¤©è¾“å…¥æ¡†
    question = st.chat_input('è¾“å…¥ä½ è¦è¯¢é—®çš„å†…å®¹')

# æ˜¾ç¤ºå†å²å¯¹è¯å†…å®¹
for message in st.session_state.history:
    with st.chat_message(message["role"]):
      st.markdown(message["content"])  

# å¤„ç†ç”¨æˆ·æé—®
if question: 
    # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    with st.chat_message("user"):
        st.markdown(question)

    # å‡†å¤‡è¯·æ±‚çŠ¶æ€
    state = []
    if st.session_state.settings["type"] == "code":
        # ä»£ç æ¨¡å¼ä½¿ç”¨ä¸“é—¨çš„ä»£ç æ¨¡å‹
        state = {"model_name": "qwen2.5-coder:7b", "messages": [{"role": "user", "content": question}], "type": "chat", "documents": []}
    else:
        # å…¶ä»–æ¨¡å¼ä½¿ç”¨é€‰æ‹©çš„æ¨¡å‹
        state = {"model_name": st.session_state.settings["model_name"], "messages": [{"role": "user", "content": question}], "type": st.session_state.settings["type"], "documents": []}

    # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
    if uploaded_file:
        state["type"] = "file"
        if not st.session_state.settings["uploaded"]:
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            file_path = upload_pdf(uploaded_file)
            # æ·»åŠ æ–‡æ¡£åˆ°è¯·æ±‚
            state["documents"].append(Document(page_content=file_path))
            st.session_state.settings["uploaded"] = True

    # è·å–AIå›ç­”å¹¶ä»¥æµå¼æ–¹å¼æ˜¾ç¤º
    answer = st.chat_message("assistant").write_stream(stream_graph_updates(st.session_state.graph, state, st.session_state.config))

    # å°†å¯¹è¯ä¿å­˜åˆ°å†å²è®°å½•
    st.session_state.history.append({"role": "user", "content": question})
    st.session_state.history.append({"role": "assistant", "content": answer})

